{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "keywords.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx7r9s0d1rAB"
      },
      "source": [
        "# Анализ новостных сообщений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fhhqYmo1rAD"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KsZHn4o1rAD"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVForXcr16k-",
        "outputId": "5458c8af-4b0e-44a2-a4a6-4a1a73706e95"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG46hTSr1rAE"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Teaching/Netology/NLP/data/lenta-ru-news.csv', usecols = ['title', 'text', 'topic', 'tags'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-iIXh7Sc1rAE",
        "outputId": "59553ffc-11e6-4202-f858-e2b83c343406"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>topic</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>800970</th>\n",
              "      <td>Шнуров раскритиковал Гагарину на «Голосе»</td>\n",
              "      <td>Певец Сергей Шнуров раскритиковал свою коллегу...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ТВ и радио</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800971</th>\n",
              "      <td>В России предложили изменить правила взыскания...</td>\n",
              "      <td>Министерство юстиции России предложило изменит...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Все</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800972</th>\n",
              "      <td>В России назвали «черную дату» для Европы</td>\n",
              "      <td>Испытание США ранее запрещенной Договором о ли...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Политика</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800973</th>\n",
              "      <td>Россиянам пообещали аномально теплую погоду</td>\n",
              "      <td>В ближайшие дни в европейской части России пог...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Общество</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800974</th>\n",
              "      <td>В конкурсе прогнозов на АПЛ разыграют 100 тыся...</td>\n",
              "      <td>Ведущие футбольные чемпионаты ушли на зимние к...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Английский футбол</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    title  ...               tags\n",
              "800970          Шнуров раскритиковал Гагарину на «Голосе»  ...         ТВ и радио\n",
              "800971  В России предложили изменить правила взыскания...  ...                Все\n",
              "800972          В России назвали «черную дату» для Европы  ...           Политика\n",
              "800973        Россиянам пообещали аномально теплую погоду  ...           Общество\n",
              "800974  В конкурсе прогнозов на АПЛ разыграют 100 тыся...  ...  Английский футбол\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7BkKVzY1rAF",
        "outputId": "0d92f3ca-9903-4857-c68f-5b7640b50eb9"
      },
      "source": [
        "df['topic'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Россия               160445\n",
              "Мир                  136621\n",
              "Экономика             79528\n",
              "Спорт                 64413\n",
              "Культура              53797\n",
              "Бывший СССР           53402\n",
              "Наука и техника       53136\n",
              "Интернет и СМИ        44663\n",
              "Из жизни              27605\n",
              "Дом                   21734\n",
              "Силовые структуры     19596\n",
              "Ценности               7766\n",
              "Бизнес                 7399\n",
              "Путешествия            6408\n",
              "69-я параллель         1268\n",
              "Крым                    666\n",
              "Культпросвет            340\n",
              "Легпром                 114\n",
              "Библиотека               65\n",
              "Оружие                    3\n",
              "ЧМ-2014                   2\n",
              "Сочи                      1\n",
              "МедНовости                1\n",
              "Name: topic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "923FQQgv1rAF",
        "outputId": "0fab9858-c792-4cc0-95a5-f7ffed5c919a"
      },
      "source": [
        "df['tags'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Все               453762\n",
              "Политика           40716\n",
              "Общество           35202\n",
              "Украина            22523\n",
              "Происшествия       19825\n",
              "                   ...  \n",
              "Нацпроекты             6\n",
              "Мировой опыт           6\n",
              "Вооружение             3\n",
              "69-я параллель         1\n",
              "Инновации              1\n",
              "Name: tags, Length: 94, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh3wf5V21rAG",
        "outputId": "a65094b7-852d-44fa-e4be-a9c7ac4f74aa"
      },
      "source": [
        "sample = df[df['tags'] == 'Кино']\n",
        "print(len(sample))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r50a4BBi1rAG"
      },
      "source": [
        "### Предобработка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jXC-hUl1rAH"
      },
      "source": [
        "#### Оставляем только слова:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M17O9uGI1rAH"
      },
      "source": [
        "import re\n",
        "regex = re.compile(\"[А-Яа-я]+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP56eF3g1rAI"
      },
      "source": [
        "def words_only(text, regex=regex):\n",
        "    return \" \".join(regex.findall(text))\n",
        "\n",
        "\n",
        "sample.text = sample.text.str.lower()\n",
        "sample.text = sample.text.apply(words_only)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "IPJZP6Lj1rAI",
        "outputId": "da23be88-2eb1-4ade-cc6e-6c9ab3301a01"
      },
      "source": [
        "sample.text.iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'американского актера уэсли снайпса в четверг приговорили к трем годам тюремного заключения за неуплату налогов в период с по годы передает агентство таким образом актер получил максимальный срок за каждый из трех эпизодов неуплаты налогов ранее со снайпса сняли более серьезные обвинения в мошенничестве и сговоре с целью ввести в заблуждение налоговую службу на судебном заседании в четверг адвокаты исполнителя роли блейда в одноименной трилогии представили три десятка писем от членов его семьи друзей и коллег актеров в которых описывался его прекрасный характер защита настаивала что снайпсу будет достаточно условного наказания так как ранее он к суду не привлекался обвинение в неуплате налогов на сумму в миллиона долларов было предъявлено снайпсу и еще двум соответчикам по делу в году как утверждали адвокаты актера он не осознавал что действует незаконно между тем приговоры в отношении соответчиков снайпса эдди рея кана и дугласа росайла будут вынесены в ближайшее время по версии обвинения кан руководил фирмой разрабатывавшей мошеннические схемы уклонения от налогов а росайл осуществлял бухгалтерскую деятельность с истекшей лицензией первому грозит до десяти лет лишения свободы а второму до шести добавляет агентство'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7Fr9uI1rAI"
      },
      "source": [
        "#### Убираем стоп-слова:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "w4rIhSao1rAJ",
        "outputId": "b25d75b7-077b-4fdf-9b6d-f4d591d9e70e"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "mystopwords = stopwords.words('russian') + ['это', 'наш' , 'тыс', 'млн', 'млрд', 'также',  'т', 'д']\n",
        "\n",
        "print(mystopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0f0e94655125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmystopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'russian'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'это'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'наш'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'тыс'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'млн'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'млрд'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'также'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'т'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'д'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmystopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtF18rh51rAK"
      },
      "source": [
        "def remove_stopwords(text, mystopwords = mystopwords):\n",
        "    try:\n",
        "        return \" \".join([token for token in text.split() if not token in mystopwords])\n",
        "    except:\n",
        "        return \"\"\n",
        "sample.text = sample.text.apply(remove_stopwords)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIGEF80G1rAK"
      },
      "source": [
        "sample.text.iloc[0][:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ut-88Kw1rAK"
      },
      "source": [
        "#### Лемматизируем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht4naRca1rAK"
      },
      "source": [
        "from pymystem3 import Mystem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD0Ypd_X1rAL"
      },
      "source": [
        "m = Mystem()\n",
        "def lemmatize(text, mystem=m):\n",
        "    try:\n",
        "        return \"\".join(m.lemmatize(text)).strip()  \n",
        "    except:\n",
        "        return \" \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGUJPYER1rAX"
      },
      "source": [
        "%%time \n",
        "sample.text = sample.text.apply(lemmatize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY7DAGiX1rAX"
      },
      "source": [
        "sample.text.iloc[0][:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V62NqnAW1rAY"
      },
      "source": [
        "#### Удаление стоп-лемм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iIJifMr1rAY"
      },
      "source": [
        "mystoplemmas = ['который', 'прошлый', 'сей', 'свой', 'наш', 'мочь', 'год']\n",
        "def  remove_stoplemmas(text, mystoplemmas = mystoplemmas):\n",
        "    try:\n",
        "        return \" \".join([token for token in text.split() if not token in mystoplemmas])\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "sample.text = sample.text.apply(remove_stoplemmas)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEzH_lHQ1rAZ"
      },
      "source": [
        "### Частотный словарь и облако слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpyEjrJ61rAZ"
      },
      "source": [
        "!pip3 install wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7-4-PyA1rAZ"
      },
      "source": [
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlymgOEB1rAZ"
      },
      "source": [
        "lemmata = [lemma for text in sample.text for lemma in text.split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVz_kN7S1rAZ"
      },
      "source": [
        "cnt = Counter(lemmata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2szt5xq81rAZ"
      },
      "source": [
        "for i in cnt.most_common(10):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chgWBInC1rAZ"
      },
      "source": [
        "word_freq = [i for i in cnt.most_common(100)]\n",
        "wd = WordCloud(background_color = 'white')\n",
        "wd.generate_from_frequencies(dict(word_freq))\n",
        "plt.figure()\n",
        "plt.imshow(wd, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRB77uUl1rAZ"
      },
      "source": [
        "## Извлечение ключевых словосочетаний\n",
        "\n",
        "\n",
        "Ключевые слова и словосочетания сложно определить формально. Поскольку определений ключевых слов и словосочетаний множество, существует масса методов их извлечения:\n",
        "* с учителем VS без учителя\n",
        "* по частотам VS посложнее\n",
        "* из одного текста VS из коллекции текстов\n",
        "* слова (униграммы) VS биграммы VS $N$-граммы\n",
        "* термины VS именованные сущности VS коллокации\n",
        "* последовательные слова VS с использованием окна"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6-CUQ5v1rAZ"
      },
      "source": [
        "### Основные этапы извлечения ключевых слов и словосочетаний:\n",
        "1. Порождение кандидатов\n",
        "2. Оценка свойст кандидатов\n",
        "3. Выбор лучших кандидатов\n",
        "\n",
        "### Основные методы извлечения ключевых слов и словосочетаний:\n",
        "* Морфологические шаблоны\n",
        "* Меры ассоциации биграмм: PMI, T-Score, LLR\n",
        "* Графовые методы: TextRank [Mihalcea, Tarau, 2004]\n",
        "* Синтаксические шаблоны\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYBAS3VS1rAZ"
      },
      "source": [
        "### Морфологические шаблоны\n",
        "\n",
        "Можно использовать парсер  Yargy. \n",
        "\n",
        "Простейший шаблон ПРИЛ + СУЩ\n",
        "\n",
        "```\n",
        "S -> Adj<gnc-agr[1]> Noun<rt,gnc-agr[1]>; \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk3Yr1jk1rAZ"
      },
      "source": [
        "### Использование мер связности \n",
        "\n",
        "\n",
        "$w_1, w_2$ − два слова\n",
        "\n",
        "$f(w_1), f(w_2)$ − их частоты\n",
        "\n",
        "$f(w_1, w_2)$ − совместная частота биграммы $w_1 w_2$\n",
        "\n",
        "$N$ − число слов\n",
        "\n",
        "$PMI(w_1, w_2) = \\log \\frac{f(w_1, w_2)}{f(w_1)f(w_2)}$\n",
        "\n",
        "$T-score(w_1, w_2) = \\frac{f(w_1,w_2)-f(w_1)*f(w_2)}{f(w_1,w_2)/N}$\n",
        "\n",
        "Другие меры связности: $\\chi^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_afLuJdu1rAZ"
      },
      "source": [
        "![chi-square](chi-square-formula.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1_InLDU1rAZ"
      },
      "source": [
        "### На практике"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaysW2cu1rAZ"
      },
      "source": [
        "Получаем из датафрейма списки по разных топиков:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NwY_9-c1rAa"
      },
      "source": [
        "def get_topic_to_tokens(df):\n",
        "    tokens_by_topic = {}\n",
        "    for topic in set(df['topic'].dropna()):\n",
        "        \n",
        "        # берём только относительно большие темы\n",
        "#         print('---')\n",
        "#         print(df['topic'])\n",
        "        if df['topic'].dropna().value_counts()[topic] > 100:\n",
        "            \n",
        "            # берём по сто случайных текстов из каждой темы\n",
        "            sample = df[df['topic']==topic].sample(n=100)\n",
        "            \n",
        "            # предобрабатываем\n",
        "            sample.text = sample.text.str.lower()\n",
        "            sample.text = sample.text.apply(words_only)\n",
        "            sample.text = sample.text.apply(remove_stopwords) \n",
        "            sample.text = sample.text.apply(lemmatize)\n",
        "            sample.text = sample.text.apply(remove_stoplemmas) \n",
        "\n",
        "            tokens_by_topic[topic] = [tok for text in sample.text for tok in text.split()]\n",
        "            \n",
        "    return tokens_by_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T0m2tjT1rAa"
      },
      "source": [
        "tokens_by_topic = get_topic_to_tokens(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MavPEbpI1rAa"
      },
      "source": [
        "tokens_by_topic.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA8X-s-r1rAa"
      },
      "source": [
        "Выберем тему, из текстов про которую будем извлекать ключевые слова:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjWFn_N91rAa"
      },
      "source": [
        "topic_texts = tokens_by_topic['Путешествия']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFZSMz7q1rAa"
      },
      "source": [
        "topic_texts[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z072pYdp1rAa"
      },
      "source": [
        "Извлекаем биграммы по разным мерам связности:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX4qn2BU1rAa"
      },
      "source": [
        "import nltk\n",
        "from nltk.collocations import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_AfHVJc1rAa"
      },
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures() # класс для мер ассоциации биграм\n",
        "finder = BigramCollocationFinder.from_words(topic_texts) # класс для хранения и извлечения биграм"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxOiObrm1rAa"
      },
      "source": [
        "N_best = 100 # число извлекаемых биграм"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrDUxY8b1rAa"
      },
      "source": [
        "%%time \n",
        "finder.apply_freq_filter(3) # избавимся от биграм, которые встречаются реже трех раз\n",
        "\n",
        "# выбираем топ-100 биграм по каждой мере\n",
        "raw_freq_ranking = [' '.join(i) for i in finder.nbest(bigram_measures.raw_freq, N_best)]\n",
        "tscore_ranking = [' '.join(i) for i in finder.nbest(bigram_measures.student_t, N_best)]\n",
        "pmi_ranking =  [' '.join(i) for i in finder.nbest(bigram_measures.pmi, N_best)]\n",
        "chi2_ranking =  [' '.join(i) for i in finder.nbest(bigram_measures.chi_sq, N_best)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5He-yKAO1rAa"
      },
      "source": [
        "Результаты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gr9_EV51rAa"
      },
      "source": [
        "rankings = pd.DataFrame({\n",
        "    'chi2': chi2_ranking,\n",
        "    't-score' : tscore_ranking,\n",
        "    'pmi': pmi_ranking,\n",
        "    'raw_freq':raw_freq_ranking\n",
        "})\n",
        "rankings = rankings[['raw_freq', 'pmi', 't-score', 'chi2']]\n",
        "rankings.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6CYH9lY1rAb"
      },
      "source": [
        "Похожи ли списки биграм? Давайте посчитаем корреляцию и визуализируем."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSsyNVT_1rAb"
      },
      "source": [
        "from scipy.stats import spearmanr\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQGWS8Uc1rAb"
      },
      "source": [
        "corr = spearmanr(rankings).correlation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xuiA9401rAb"
      },
      "source": [
        "corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKwj8t11rAb"
      },
      "source": [
        "sns.heatmap(corr, annot=True, xticklabels = list(rankings), yticklabels = list(rankings))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lqJUuNr1rAb"
      },
      "source": [
        "Если у нас есть данные про время, можно считать \"трендовые слова\": те слова, ранг которых по частотности вырос по сравнению с предыдущим периодом.\n",
        "\n",
        "#### Задание\n",
        "\n",
        "Возьмите две главны какой-нибудь большой книжки (или другие текстовые данные, в которых что-то известно про время создания), и найдите трендовые слова используя разницу в порядке слова по частотности."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeZLOf0O1rAb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVmB-QUn1rAb"
      },
      "source": [
        "### Графовые методы\n",
        "\n",
        "* Вершины графа: слова\n",
        "* Ребра графа могут определяться по следующим правилам:\n",
        "    * Последовательные слова\n",
        "    * Слова внутри левого или правого окна в $\\pm$ 2-5 слов  \n",
        "\n",
        "* Ребра могут быть взвешенные или невзвешенные, направленные или ненаправленные\n",
        "* Любая мера центральности графа используется для определения важности вершин в графе. Слова, соответствующие наиболее важным вершинам, считаются ключевыми. \n",
        "* Если две соседние вершины оказываются важными, соответствующие им слова формируют ключевое словосочетание."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk2srmem1rAb"
      },
      "source": [
        "Меры центральностей.\n",
        "![centralities](centrality_measures.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A7WQWc41rAb"
      },
      "source": [
        "A) Betweenness centrality\n",
        "\n",
        "B) Closeness centrality\n",
        "\n",
        "C) Eigenvector centrality\n",
        "\n",
        "D) Degree centrality\n",
        "\n",
        "E) Harmonic centrality\n",
        "\n",
        "F) Katz centrality of the same graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUGxbVLR1rAb"
      },
      "source": [
        "#### TextRank\n",
        "\n",
        "Работает по тому же принципу, что и PageRank. Рёбра -- совстречаемость слов.\n",
        "\n",
        "![PageRank](PageRank.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpWVURPm1rAb"
      },
      "source": [
        "Используем TextRank для извлечения ключевых слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYjO-39Q1rAb"
      },
      "source": [
        "!pip3 install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGYWnbhl1rAb"
      },
      "source": [
        "from gensim.summarization import keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrsJECKC1rAb"
      },
      "source": [
        "%%time\n",
        "text = ' '.join(topic_texts)\n",
        "kw = keywords(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVBIZaUj1rAb"
      },
      "source": [
        "Результаты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLG7ikMj1rAb"
      },
      "source": [
        "rankings = pd.DataFrame({'Text Rank': kw.split('\\n')})\n",
        "rankings.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtoIojXX1rAb"
      },
      "source": [
        "## Мера контрастности $tf-idf$\n",
        "\n",
        "\n",
        "\n",
        "Частота терма [Luhn, 1957]:  Важность терма в тексте пропорциональная его частоте.\n",
        "\n",
        "Обратная документная частота [Spaerck Jones, 1972]: Специфичность терма в тексте обратно пропорциональна числу текстов, в которых терм встречается. \n",
        "\n",
        "$tfidf(term, text, collection) = tf(term, document) \\times idf(term, collection)$\n",
        "\n",
        "Самая популярная комбинация весов: $f_{t,d} \\times \\log \\frac{|D|}{n_t+1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qGZ4HO91rAc"
      },
      "source": [
        "Извлекаем ключевые слова по $tf-idf$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gax6nCIm1rAc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOEkHhCD1rAc"
      },
      "source": [
        "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0)\n",
        "tfidf_matrix =  tfidf.fit_transform([' '.join(tokens) for topic, tokens in tokens_by_topic.items()])\n",
        "feature_names = tfidf.get_feature_names() \n",
        "dense = tfidf_matrix.todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "779HBDXk1rAc"
      },
      "source": [
        "topic_id = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVbGl3zT1rAc"
      },
      "source": [
        "text = dense[topic_id].tolist()[0]\n",
        "phrase_scores = [pair for pair in zip(range(0, len(text)), text) if pair[1] > 0]\n",
        "sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOf-0M7o1rAc"
      },
      "source": [
        "tfidf_ranking = []\n",
        "for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:40]:\n",
        "    tfidf_ranking.append(phrase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb1uNlh61rAc"
      },
      "source": [
        "Результаты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-bKyFZH1rAc"
      },
      "source": [
        "rankings = pd.DataFrame({'tf-idf': tfidf_ranking})\n",
        "rankings.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}