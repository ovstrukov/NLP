{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypt7ymI8X-rM"
   },
   "source": [
    "# Embeddings  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1VGqw8yMPsFmlH5IHv8Vca31OVaikcgrh?usp=sharing)\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "Векторные модели, которые мы рассматривали до этого (tf-idf, BOW), условно называются *счётными*. Они основываются на том, что так или иначе \"считают\" слова и их соседей, и на основе этого строят вектора для слов. \n",
    "\n",
    "Другой класс моделей, который более повсевмёстно распространён на сегодняшний день, называется *предсказательными* (или *нейронными*) моделями. Идея этих моделей заключается в использовании нейросетевых архитектур, которые \"предсказывают\" (а не считают) соседей слов. Одной из самых известных таких моделей является word2vec. Технология основана на нейронной сети, предсказывающей вероятность встретить слово в заданном контексте. Этот инструмент был разработан группой исследователей Google в 2013 году, руководителем проекта был Томаш Миколов (сейчас работает в Facebook). Вот две самые главные статьи:\n",
    "\n",
    "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "\n",
    "\n",
    "Полученные таким образом вектора называются *распределенными представлениями слов*, или **эмбеддингами**.\n",
    "\n",
    "\n",
    "### Как это обучается?\n",
    "Мы задаём вектор для каждого слова с помощью матрицы $w$ и вектор контекста с помощью матрицы $W$. По сути, word2vec является обобщающим названием для двух архитектур Skip-Gram и Continuous Bag-Of-Words (CBOW).  \n",
    "\n",
    "**CBOW** предсказывает текущее слово, исходя из окружающего его контекста. \n",
    "\n",
    "**Skip-gram**, наоборот, использует текущее слово, чтобы предугадывать окружающие его слова. \n",
    "\n",
    "### Как это работает?\n",
    "Word2vec принимает большой текстовый корпус в качестве входных данных и сопоставляет каждому слову вектор, выдавая координаты слов на выходе. Сначала он создает словарь, «обучаясь» на входных текстовых данных, а затем вычисляет векторное представление слов. Векторное представление основывается на контекстной близости: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, согласно дистрибутивной гипотезе, имеющие схожий смысл), в векторном представлении будут иметь близкие координаты векторов-слов. Для вычисления близости слов используется косинусное расстояние между их векторами.\n",
    "\n",
    "\n",
    "С помощью дистрибутивных векторных моделей можно строить семантические пропорции (они же аналогии) и решать примеры:\n",
    "\n",
    "* *король: мужчина = королева: женщина* \n",
    " $\\Rightarrow$ \n",
    "* *король - мужчина + женщина = королева*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3sSyfHaX-rN"
   },
   "source": [
    "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oH_UDyFX-rO"
   },
   "source": [
    "### Проблемы\n",
    "Невозможно установить тип семантических отношений между словами: синонимы, антонимы и т.д. будут одинаково близки, потому что обычно употребляются в схожих контекстах. Поэтому близкие в векторном пространстве слова называют *семантическими ассоциатами*. Это значит, что они семантически связаны, но как именно — непонятно.\n",
    "\n",
    "\n",
    "### RusVectōrēs\n",
    "\n",
    "\n",
    "На сайте [RusVectōrēs](https://rusvectores.org/ru/) собраны предобученные на различных данных модели для русского языка, а также можно поискать наиболее близкие слова к заданному, посчитать семантическую близость нескольких слов и порешать примеры с помощью «калькулятором семантической близости».\n",
    "\n",
    "\n",
    "Для других языков также можно найти предобученные модели — например, модели [fastText](https://fasttext.cc/docs/en/english-vectors.html) и [GloVe](https://nlp.stanford.edu/projects/glove/) (о них чуть дальше).\n",
    "\n",
    "### Визуализация\n",
    "А [вот тут](https://projector.tensorflow.org/) есть хорошая визуализация для английского."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXNm8flVX-rP"
   },
   "source": [
    "## Gensim\n",
    "\n",
    "Использовать предобученную модель эмбеддингов или обучить свою можно с помощью библиотеки `gensim`. Вот [ее документация](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "### Как использовать готовую модель\n",
    "\n",
    "Модели word2vec бывают разных форматов:\n",
    "\n",
    "* .vec.gz — обычный файл\n",
    "* .bin.gz — бинарник\n",
    "\n",
    "Загружаются они с помощью одного и того же класса `KeyedVectors`, меняется только параметр `binary` у функции `load_word2vec_format`. \n",
    "\n",
    "Если же эмбеддинги обучены **не** с помощью word2vec, то для загрузки нужно использовать функцию `load`. Т.е. для загрузки предобученных эмбеддингов *glove, fasttext, bpe* и любых других нужна именно она.\n",
    "\n",
    "Скачаем с RusVectōrēs модель для русского языка, обученную на НКРЯ образца 2015 г. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from gensim) (1.18.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/Users/ktulhu/maryszmary_ve/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=c0309604d5a40f94ad16575ce7fe562f4a49df661d4c9c79a52f9328d3c41b0e\n",
      "  Stored in directory: /Users/ktulhu/Library/Caches/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.10.0 bs4-0.0.1 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/Users/ktulhu/maryszmary_ve/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "sTLnNKaNX-rP",
    "outputId": "2a1f013e-b57d-4984-9c73-bc64e5a0e816"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ktulhu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data \n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "zm9z6SN3X-rS",
    "outputId": "e3280992-dadc-40a2-c356-547cf399e599"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ruscorpora_mystem_cbow_300_2_2015.bin.gz',\n",
       " <http.client.HTTPMessage at 0x104536430>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "mkuCawyFX-rW",
    "outputId": "654509e9-9186-4af1-bea2-ab5962202014"
   },
   "outputs": [],
   "source": [
    "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
    "\n",
    "model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvvO1ew5p3PP"
   },
   "source": [
    "Возьмем несколько слов для примера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H4D0hMgkX-rY"
   },
   "outputs": [],
   "source": [
    "words = ['день_S', 'ночь_S', 'человек_S', 'семантика_S', 'биткоин_S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbTShU9aX-ra"
   },
   "source": [
    "Частеречные тэги нужны, поскольку это специфика скачанной модели - она была натренирована на словах, аннотированных их частями речи (и лемматизированных). **NB!** В названиях моделей на `rusvectores` указано, какой тегсет они используют (mystem, upos и т.д.)\n",
    "\n",
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Uer5sHyX-ra",
    "outputId": "901d7be2-77e6-4906-a5f8-672e7efa1d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_S\n",
      "[-0.02580778  0.00970898  0.01941961 -0.02332282  0.02017624  0.07275085\n",
      " -0.01444375  0.03316632  0.01242602  0.02833412]\n",
      "неделя_S :  0.7165195941925049\n",
      "месяц_S :  0.631048858165741\n",
      "вечер_S :  0.5828739404678345\n",
      "утро_S :  0.5676207542419434\n",
      "час_S :  0.5605547428131104\n",
      "минута_S :  0.5297018885612488\n",
      "гекатомбеон_S :  0.4897990822792053\n",
      "денек_S :  0.48224714398384094\n",
      "полчаса_S :  0.48217129707336426\n",
      "ночь_S :  0.478074848651886\n",
      "\n",
      "\n",
      "ночь_S\n",
      "[-0.00688948  0.00408364  0.06975466 -0.00959525  0.0194835   0.04057068\n",
      " -0.00994112  0.06064967 -0.00522624  0.00520327]\n",
      "вечер_S :  0.6946247816085815\n",
      "утро_S :  0.57301926612854\n",
      "ноченька_S :  0.5582467317581177\n",
      "рассвет_S :  0.555358350276947\n",
      "ночка_S :  0.5351512432098389\n",
      "полдень_S :  0.5334426164627075\n",
      "полночь_S :  0.478694349527359\n",
      "день_S :  0.4780748784542084\n",
      "сумерки_S :  0.4390218257904053\n",
      "фундерфун_S :  0.4340824782848358\n",
      "\n",
      "\n",
      "человек_S\n",
      "[ 0.02013756 -0.02670703 -0.02039861 -0.05477146  0.00086402 -0.01636335\n",
      "  0.04240306 -0.00025525 -0.14045681  0.04785006]\n",
      "женщина_S :  0.5979775190353394\n",
      "парень_S :  0.4991787374019623\n",
      "мужчина_S :  0.4767409563064575\n",
      "мужик_S :  0.47384002804756165\n",
      "россиянин_S :  0.47190436720848083\n",
      "народ_S :  0.4654741883277893\n",
      "согражданин_S :  0.45378512144088745\n",
      "горожанин_S :  0.44368088245391846\n",
      "девушка_S :  0.44314485788345337\n",
      "иностранец_S :  0.43849867582321167\n",
      "\n",
      "\n",
      "семантика_S\n",
      "[-0.03066749  0.0053851   0.1110732   0.0152335   0.00440643  0.00384104\n",
      "  0.00096944 -0.03538784 -0.00079585  0.03220548]\n",
      "семантический_A :  0.5334584712982178\n",
      "понятие_S :  0.5030269026756287\n",
      "сочетаемость_S :  0.4817051291465759\n",
      "актант_S :  0.47596412897109985\n",
      "хронотоп_S :  0.46330299973487854\n",
      "метафора_S :  0.46158894896507263\n",
      "мышление_S :  0.4610119163990021\n",
      "парадигма_S :  0.45796656608581543\n",
      "лексема_S :  0.45688074827194214\n",
      "смысловой_A :  0.4543077349662781\n",
      "\n",
      "\n",
      "Увы, слова \"биткоин_S\" нет в модели!\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? \n",
    "    if word in model_ru:\n",
    "        print(word)\n",
    "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
    "        print(model_ru[word][:10])\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for word, sim in model_ru.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(word, ': ', sim)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print('Увы, слова \"%s\" нет в модели!' % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GqDmAcJX-rc"
   },
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "MDFjOSJjX-rd",
    "outputId": "364ab495-67ff-49fb-964d-6324343df18d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23895611\n"
     ]
    }
   ],
   "source": [
    "print(model_ru.similarity('человек_S', 'обезьяна_S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0w4pQooX-rf"
   },
   "source": [
    "Что получится, если вычесть из пиццы Италию и прибавить Сибирь?\n",
    "\n",
    "* positive — вектора, которые мы складываем\n",
    "* negative — вектора, которые вычитаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "N0L5_TCQX-rf",
    "outputId": "b4019ddf-80a8-41e5-baa2-146c5b5d7c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пельмень_S\n"
     ]
    }
   ],
   "source": [
    "print(model_ru.most_similar(positive=['пицца_S', 'сибирь_S'], negative=['италия_S'])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "DrN2Jc31X-rh",
    "outputId": "295b7a9c-3a2a-4a80-fde1-d78df9c81e0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ананас_S'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ru.doesnt_match('пицца_S пельмень_S хот-дог_S ананас_S'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9XsGSE_HuFJ"
   },
   "source": [
    "**Упражнения для разминки**\n",
    "\n",
    "Найдите пример многозначного слова, для которого в топ-10 (метод `most_similar`) похожих на него слов входят слова связанные с разными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWXGH79-H1ED"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adiAA7AqIY4g"
   },
   "source": [
    "По аналогии с Италия -- пицца, Сибирь -- пельмень, придумайте похожую связку слов для проверки: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5bzD-ZXKOSY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpIDtQIzKO1M"
   },
   "source": [
    "Приведите пример трех слов w1, w2, w3, таких, что w1 и w2 являются синонимами, w1 и w3 являются антонимами, но при этом, similarity(w1, w2) < similarity(w1, w3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFXaxVhlKPb2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rfihwVJKwzC"
   },
   "source": [
    "### Задание\n",
    "\n",
    "Напишите функцию, которая принимает на вход предложение, и заменяет случайное существительное в нём на \"ассоциат\" -- ближайшее к нему слово из модели word2vec.\n",
    "\n",
    "NB: для этого вам понадобится морфологический анализатор. Советую использовать pymorphy (мы кратко говорили про него на прошлом семинаре)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCMPmoRrM3ES"
   },
   "source": [
    " как пользоваться pymorphy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "BHf_0AeCKxIs",
    "outputId": "e026df1b-756c-469e-ca9b-cdd6d88b457d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: docopt>=0.6 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from pymorphy2) (0.7.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/Users/ktulhu/maryszmary_ve/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uRWeAurhM7IW"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AYvl-ttDNCJl"
   },
   "outputs": [],
   "source": [
    "analyser = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "xUvP-U-nNF6Y",
    "outputId": "52621d20-d246-4795-9b65-0129cefe0150"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='слово', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='слово', score=0.59813, methods_stack=((DictionaryAnalyzer(), 'слово', 54, 0),)),\n",
       " Parse(word='слово', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='слово', score=0.401869, methods_stack=((DictionaryAnalyzer(), 'слово', 54, 3),))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# разобрать слово (в данном случае возможно два разбора, поэтому получаем список из двух элементов)\n",
    "result = analyser.parse('слово')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "enrQF-SQOtQP",
    "outputId": "f21bf374-56a6-45fa-89a1-368906d9a797"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# достать часть речи\n",
    "result[0].tag.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "a8P6VpjDPR_z",
    "outputId": "b1d91898-db84-4405-d7c7-38ca3b02ea87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'слову'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# поставить в дательный падеж\n",
    "result[0].inflect(frozenset(['datv'])).word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNhjJC-xPwMg"
   },
   "source": [
    "Ваша функция (для простоты можно не пытаться поставить слово в \"нужную\" форму и ограничиться именительным падежом):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5slDKGtP23t"
   },
   "outputs": [],
   "source": [
    "def change_random_noun(sentence):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNHMtmbqX-rl"
   },
   "source": [
    "## Как обучить свою модель\n",
    "\n",
    "В качестве обучающих данных возьмем размеченные и неразмеченные отзывы о фильмах (датасет взят с Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "-1Vz9le-X-rl",
    "outputId": "698576f8-edab-43c5-ee3a-0c2c295fefa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to ru_GB.\n",
      "Warning: Failed to set locale category LC_TIME to ru_GB.\n",
      "Warning: Failed to set locale category LC_COLLATE to ru_GB.\n",
      "Warning: Failed to set locale category LC_MONETARY to ru_GB.\n",
      "Warning: Failed to set locale category LC_MESSAGES to ru_GB.\n",
      "--2021-10-15 13:39:46--  https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/unlabeledTrainData.tsv\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 67281491 (64M) [text/plain]\n",
      "Сохранение в: «unlabeledTrainData.tsv»\n",
      "\n",
      "unlabeledTrainData. 100%[===================>]  64.16M  7.41MB/s    за 9.1s    \n",
      "\n",
      "2021-10-15 13:39:58 (7.07 MB/s) - «unlabeledTrainData.tsv» сохранён [67281491/67281491]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/unlabeledTrainData.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B2wv-vSxX-ro"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "eEGmrh-nX-rq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIZaQ3kYX-rt"
   },
   "source": [
    "Убираем из данных ссылки, html-разметку и небуквенные символы, а затем приводим все к нижнему регистру и токенизируем. На выходе получается массив из предложений, каждое из которых представляет собой массив слов. Здесь используется токенизатор из библиотеки `nltk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZAzxyk3WTgrd"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "irbunSOfX-rt"
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False ):\n",
    "    # убираем ссылки\n",
    "    review_text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
    "    # достаем сам текст\n",
    "#     review_text = BeautifulSoup(review_text, \"lxml\").get_text()\n",
    "    # оставляем только буквенные символы\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # приводим к нижнему регистру и разбиваем на слова по символу пробела\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords: # убираем стоп-слова\n",
    "        stops = stopwords.words(\"english\")\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # разбиваем обзор на предложения\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # применяем предыдущую функцию к каждому предложению\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "bolPEuQgX-rv",
    "outputId": "9a9a76b4-5d61-46ce-a72c-c25f2bbda75d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 192/50000 [00:00<00:26, 1913.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:23<00:00, 2111.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = []  \n",
    "\n",
    "print(\"Parsing sentences from training set...\")\n",
    "for review in tqdm(data[\"review\"]):\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "FbrH6TdfX-rx",
    "outputId": "2122347d-0eaf-4cb4-c025-e96976360ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528987\n",
      "['watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Cqb_NOteX-rz"
   },
   "outputs": [],
   "source": [
    "# это понадобится нам позже\n",
    "\n",
    "with open('clean_text.txt', 'w') as f:\n",
    "    for s in sentences[:5000]:\n",
    "        f.write(' '.join(s))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqFhuv7XX-r1"
   },
   "source": [
    "Обучаем и сохраняем модель. \n",
    "\n",
    "\n",
    "Основные параметры:\n",
    "* данные должны быть итерируемым объектом \n",
    "* size — размер вектора, \n",
    "* window — размер окна наблюдения,\n",
    "* min_count — мин. частотность слова в корпусе,\n",
    "* sg — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram),\n",
    "* sample — порог для downsampling'a высокочастотных слов,\n",
    "* workers — количество потоков,\n",
    "* alpha — learning rate,\n",
    "* iter — количество итераций,\n",
    "* max_vocab_size — позволяет выставить ограничение по памяти при создании словаря (т.е. если ограничение превышается, то низкочастотные слова будут выбрасываться). Для сравнения: 10 млн слов = 1Гб RAM.\n",
    "\n",
    "**NB!** Обратите внимание, что тренировка модели не включает препроцессинг! Это значит, что избавляться от пунктуации, приводить слова к нижнему регистру, лемматизировать их, проставлять частеречные теги придется до тренировки модели (если, конечно, это необходимо для вашей задачи). Т.е. в каком виде слова будут в исходном тексте, в таком они будут и в модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "OB2FqIioX-r1",
    "outputId": "7a8840a9-a12b-4247-bf6b-b02c767ce2c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "CPU times: user 2min 23s, sys: 565 ms, total: 2min 24s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model_en = word2vec.Word2Vec(sentences, workers=4, vector_size=300, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFU6SYAZX-r3"
   },
   "source": [
    "Смотрим, сколько в модели слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HA3P0CUYX-r4",
    "outputId": "1bfec39d-aa30-4a4e-b77c-e8ed9c8c2571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28316\n"
     ]
    }
   ],
   "source": [
    "print(len(model_en.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mU9sbGYX-r8"
   },
   "source": [
    "Попробуем оценить модель вручную, порешав примеры. Несколько дано ниже, попробуйте придумать свои."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "n8OQxmUyX-r9",
    "outputId": "4e4ab20e-31e4-4c02-fdfa-61b4ba8be133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actress', 0.7792311310768127)]\n",
      "[('men', 0.6633631587028503)]\n",
      "[('europe', 0.7178550958633423), ('canada', 0.6989084482192993), ('australia', 0.6931552886962891)]\n",
      "novel\n"
     ]
    }
   ],
   "source": [
    "print(model_en.wv.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))\n",
    "print(model_en.wv.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))\n",
    "\n",
    "print(model_en.wv.most_similar(\"usa\", topn=3))\n",
    "\n",
    "print(model_en.wv.doesnt_match(\"comedy thriller western novel\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juR4NX0wX-r_"
   },
   "source": [
    "### Как дообучить существующую модель\n",
    "\n",
    "При тренировке модели \"с нуля\" веса инициализируются случайно, однако можно использовать для инициализации векторов веса из предобученной модели, таким образом как бы дообучая ее.\n",
    "\n",
    "Сначала посмотрим близость какой-нибудь пары слов в имеющейся модели, чтобы потом сравнить результат с дообученной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "rAUVp96kX-sA",
    "outputId": "610c4080-a724-40e5-dc77-30f83bd46cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2506585"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.wv.similarity('lion', 'rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxsz8DKMX-sD"
   },
   "source": [
    "В качестве дополнительных данных для обучения возьмем английский текст «Алисы в Зазеркалье»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "4eTCOgpMX-sE",
    "outputId": "6ed0a4a9-c6a2-4f3f-e37c-d88e4cb42e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to ru_GB.\n",
      "Warning: Failed to set locale category LC_TIME to ru_GB.\n",
      "Warning: Failed to set locale category LC_COLLATE to ru_GB.\n",
      "Warning: Failed to set locale category LC_MONETARY to ru_GB.\n",
      "Warning: Failed to set locale category LC_MESSAGES to ru_GB.\n",
      "--2021-10-15 13:47:49--  https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/alice.txt\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 167631 (164K) [text/plain]\n",
      "Сохранение в: «alice.txt»\n",
      "\n",
      "alice.txt           100%[===================>] 163.70K  --.-KB/s    за 0.1s    \n",
      "\n",
      "2021-10-15 13:47:50 (1.45 MB/s) - «alice.txt» сохранён [167631/167631]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "yHuXJgB_X-sI",
    "outputId": "ee69966b-8428-4172-ba4c-7669a57465ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['through', 'the', 'looking-glass', 'by', 'lewis', 'carroll', 'chapter', 'i', 'looking-glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', '', 'it', 'was', 'the', 'black', 'kitten’s', 'fault', 'entirely'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', 'and', 'bearing', 'it', 'pretty', 'well', 'considering', 'so', 'you', 'see', 'that', 'it', 'couldn’t', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# убираем переносы строк, токенизируем текст\n",
    "text = re.sub('\\n', ' ', text)\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "# убираем всю пунктуацию и делим текст на слова по пробелу\n",
    "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~„“«»†*—/\\-‘’'\n",
    "clean_sents = []\n",
    "for sent in sents:\n",
    "    s = [w.lower().strip(punct) for w in sent.split()]\n",
    "    clean_sents.append(s)\n",
    "    \n",
    "print(clean_sents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH8f7GNBX-sK"
   },
   "source": [
    "Чтобы дообучить модель, надо сначала ее сохранить, а потом загрузить. Все параметры тренировки (размер вектора, мин. частота слова и т.п.) будут взяты из загруженной модели, т.е. задать их заново нельзя.\n",
    "\n",
    "**NB!** Дообучить можно только полную модель, а `KeyedVectors` — нельзя. Поэтому сохранять модель нужно в соотвествующем формате. Подробнее о разнице [вот тут](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Rv--44TmX-sL",
    "outputId": "0973b4a0-dcbe-498e-86a5-28f6e9673aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"movie_reviews.model\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_en.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "f6eZwBQzX-sQ",
    "outputId": "be412d2b-8cc7-424b-9251-bc2aedb1474d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97075, 150225)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "model.build_vocab(clean_sents, update=True)\n",
    "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMLhISWiX-sS"
   },
   "source": [
    "Лев и кролик стали чуть ближе друг к другу!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "4USaUjmzX-sS",
    "outputId": "5f20a83c-b50e-4ca3-bd28-ddb8e16e319e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25955093"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('lion', 'rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5bYd1qmX-sT"
   },
   "source": [
    "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать. Здесь используется L2-нормализация: вектора нормализуются так, что если сложить квадраты всех элементов вектора, в сумме получится 1. \n",
    "\n",
    "Кроме того, сохраним не полные вектора, а `KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "ZAVaTYvQX-sU",
    "outputId": "4c2ef602-996f-4ef0-9a77-55a53cabccb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)\n",
    "model_path = \"movies_alice.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_en.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXKnPffPX-sY"
   },
   "source": [
    "## Оценка\n",
    "\n",
    "Это, конечно, хорошо, но как понять, какая модель лучше? Или вот, например, я сделал свою модель, а как понять, насколько она хорошая?\n",
    "\n",
    "Для этого существуют специальные датасеты для оценки качества дистрибутивных моделей. Основных два: один измеряет точность решения задач на аналогии (про Россию и пельмени), а второй используется для оценки коэффициента семантической близости. \n",
    "\n",
    "### Word Similarity\n",
    "\n",
    "Этот метод заключается в том, чтобы оценить, насколько представления о семантической близости слов в модели соотносятся с \"представлениями\" людей.\n",
    "\n",
    "| слово 1    | слово 2    | близость | \n",
    "|------------|------------|----------|\n",
    "| кошка      | собака     | 0.7      |  \n",
    "| чашка      | кружка     | 0.9      |       \n",
    "\n",
    "Для каждой пары слов из заранее заданного датасета мы можем посчитать косинусное расстояние, и получить список таких значений близости. При этом у нас уже есть список значений близостей, сделанный людьми. Мы можем сравнить эти два списка и понять, насколько они похожи (например, посчитав корреляцию). Эта мера схожести должна говорить о том, насколько модель хорошо моделирует расстояния до слова.\n",
    "\n",
    "### Аналогии\n",
    "\n",
    "Другая популярная задача для \"внутренней\" оценки называется задачей поиска аналогий. Как мы уже разбирали выше, с помощью простых арифметических операций мы можем модифицировать значение слова. Если заранее собрать набор слов-модификаторов, а также слов, которые мы хотим получить в результаты модификации, то на основе подсчёта количества \"попаданий\" в желаемое слово мы можем оценить, насколько хорошо работает модель.\n",
    "\n",
    "В качестве слов-модификаторов мы можем использовать семантические аналогии. Скажем, если у нас есть некоторое отношение \"страна-столица\", то для оценки модели мы можем использовать пары наподобие \"Россия-Москва\", \"Норвегия-Осло\", и т.д. Датасет будет выглядеть следующм образом:\n",
    "\n",
    "| слово 1    | слово 2    | отношение     | \n",
    "|------------|------------|---------------|\n",
    "| Россия     | Москва     | страна-столица|  \n",
    "| Норвегия   | Осло       | страна-столица|\n",
    "\n",
    "Рассматривая случайные две пары из этого набора, мы хотим, имея триплет (Россия, Москва, Норвегия) хотим получить слово \"Осло\", т.е. найти такое слово, которое будет находиться в том же отношении со словом \"Норвегия\", как \"Россия\" находится с Москвой. \n",
    "\n",
    "Датасеты для русского языка можно скачать на странице с моделями на RusVectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to ru_GB.\n",
      "Warning: Failed to set locale category LC_TIME to ru_GB.\n",
      "Warning: Failed to set locale category LC_COLLATE to ru_GB.\n",
      "Warning: Failed to set locale category LC_MONETARY to ru_GB.\n",
      "Warning: Failed to set locale category LC_MESSAGES to ru_GB.\n",
      "--2021-10-15 13:50:10--  https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/evaluation/ru_analogy_tagged.txt\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 871776 (851K) [text/plain]\n",
      "Сохранение в: «ru_analogy_tagged.txt.1»\n",
      "\n",
      "ru_analogy_tagged.t 100%[===================>] 851.34K  4.62MB/s    за 0.2s    \n",
      "\n",
      "2021-10-15 13:50:10 (4.62 MB/s) - «ru_analogy_tagged.txt.1» сохранён [871776/871776]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/evaluation/ru_analogy_tagged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": capital-common-countries\r\n",
      "афины_S греция_S багдад_S ирак_S\r\n",
      "афины_S греция_S бангкок_S таиланд_S\r\n",
      "афины_S греция_S пекин_S китай_S\r\n",
      "афины_S греция_S берлин_S германия_S\r\n",
      "афины_S греция_S берн_S швейцария_S\r\n",
      "афины_S греция_S каир_S египет_S\r\n",
      "афины_S греция_S канберра_S австралия_S\r\n",
      "афины_S греция_S ханой_S вьетнам_S\r\n",
      "афины_S греция_S гавана_S куба_S\r\n"
     ]
    }
   ],
   "source": [
    "!head ru_analogy_tagged.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qZcF3nGX-sp"
   },
   "source": [
    "## FastText\n",
    "\n",
    "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грам. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки.\n",
    "\n",
    "* [Статья](https://aclweb.org/anthology/Q17-1010)\n",
    "* [Сайт](https://fasttext.cc/)\n",
    "* [Тьюториал](https://fasttext.cc/docs/en/support.html)\n",
    "* [Вектора для 157 языков](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "* [Вектора, обученные на википедии](https://fasttext.cc/docs/en/pretrained-vectors.html) (отдельно для 294 разных языков)\n",
    "* [Репозиторий](https://github.com/facebookresearch/fasttext)\n",
    "\n",
    "Есть библиотека `fasttext` для питона (с готовыми моделями можно работать и через `gensim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "NvzM2AL7Z8gl",
    "outputId": "a93516dc-c859-41f7-e922-2ebd06eadd6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клонирование в «fastText»…\n",
      "remote: Enumerating objects: 3854, done.\u001b[K\n",
      "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
      "Получение объектов: 100% (3854/3854), 8.22 MiB | 938.00 KiB/s, готово.\n",
      "Определение изменений: 100% (2417/2417), готово.\n",
      "Processing ./fastText\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from fasttext==0.9.2) (2.8.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from fasttext==0.9.2) (46.1.3)\n",
      "Requirement already satisfied: numpy in /Users/ktulhu/maryszmary_ve/lib/python3.8/site-packages (from fasttext==0.9.2) (1.18.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-macosx_10_14_x86_64.whl size=344360 sha256=005bcadd1cc6df9a6f8d8c6326dc788793fc50eb5a825f79d0257b2d7e38dcdc\n",
      "  Stored in directory: /private/var/folders/zy/214v8d1d4tng0_jtxf69crf40000gp/T/pip-ephem-wheel-cache-zd8oz4ct/wheels/f0/7c/da/fe73eff2b10eada58ea862d143fc14e2197ecb05a3545ae5ac\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "Successfully installed fasttext-0.9.2\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/Users/ktulhu/maryszmary_ve/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/facebookresearch/fastText.git\n",
    "! pip3 install fastText/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "s-IW_cBDX-sp"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# так можно обучить свою модель \n",
    "ft_model = fasttext.train_unsupervised('clean_text.txt', minn=3, maxn=4, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NNiuzf4DX-sr",
    "outputId": "944550f0-33d5-498b-f16d-7b9872667338"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1313279 , -0.02241677, -0.18896143, -0.06513993, -0.09668837,\n",
       "        0.06699901, -0.17133492, -0.08639518, -0.1176882 ,  0.04813271],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.get_word_vector(\"movie\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "TsZnXqH8X-st",
    "outputId": "983fcc63-9a13-43fb-a4be-3dfd55d70863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9998166561126709, 'actors'),\n",
       " (0.9997903108596802, 'actresses'),\n",
       " (0.9996681213378906, 'presence'),\n",
       " (0.9996576309204102, 'perfect'),\n",
       " (0.9996448755264282, 'act'),\n",
       " (0.9996447563171387, 'actual'),\n",
       " (0.9996442794799805, 'director'),\n",
       " (0.9996076226234436, 'influence'),\n",
       " (0.9996068477630615, 'prince'),\n",
       " (0.9995878338813782, 'violence')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.get_nearest_neighbors('actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "ttAgMPC_X-sx",
    "outputId": "9bbec1ad-d83a-4e75-df98-94356598a671"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9996215105056763, 'actor'),\n",
       " (0.999507486820221, 'act'),\n",
       " (0.9994754791259766, 'acts'),\n",
       " (0.9994434118270874, 'actors'),\n",
       " (0.9993497133255005, 'actual'),\n",
       " (0.9992961287498474, 'actresses'),\n",
       " (0.9992280006408691, 'actions'),\n",
       " (0.9992159008979797, 'perfect'),\n",
       " (0.9991544485092163, 'director'),\n",
       " (0.9991104602813721, 'perfectly')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проблема с опечатками решена\n",
    "\n",
    "ft_model.get_nearest_neighbors('actr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "V9alEdz0X-s0",
    "outputId": "1126182a-8abd-471c-9346-59e852625c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9996706247329712, 'reviews'),\n",
       " (0.9996266961097717, 'just'),\n",
       " (0.9996209144592285, 'movie'),\n",
       " (0.9996070265769958, 'watchable'),\n",
       " (0.999596357345581, 'move'),\n",
       " (0.9995846748352051, 'probably'),\n",
       " (0.9995819330215454, 'review'),\n",
       " (0.9995530843734741, 'views'),\n",
       " (0.9995368123054504, 'rented'),\n",
       " (0.9995203614234924, 'realise')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проблема с out of vocabulary словами - тоже\n",
    "\n",
    "ft_model.get_nearest_neighbors('moviegeek')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "0f5FRfEMX-s3"
   },
   "source": [
    "Кроме этого, fastText можно использовать для классификации, для этого нужен следующий формат размеченных данных:\n",
    "\n",
    "__label_1__  text_1\n",
    "\n",
    "__label_2__  text_2\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../sem1/tweets_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>мыс на меня обиделась:(\\nя ей даже ничего не с...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>аааааааааааааааааааа,не хочу на работу :(</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>У меня какой-то особенный вид ушей! :D, некото...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@simonovkon  он неплохой человек в жизни. Я ра...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Darina_Lo: Домааааа\\nЕхали на такси, пели ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  мыс на меня обиделась:(\\nя ей даже ничего не с...  negative\n",
       "1          аааааааааааааааааааа,не хочу на работу :(  negative\n",
       "2  У меня какой-то особенный вид ушей! :D, некото...  positive\n",
       "3  @simonovkon  он неплохой человек в жизни. Я ра...  negative\n",
       "4  RT @Darina_Lo: Домааааа\\nЕхали на такси, пели ...  positive"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-YB6B4CvmNJ8",
    "outputId": "4f50acfa-27c4-4a50-ddcb-612fd5cfc85d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226834"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QuPeLrvX-tI"
   },
   "source": [
    "Запишем полученные данные в формате для обучения классификатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "AGOFQSekX-tI",
    "outputId": "0eba65f3-d4b1-4b38-e31a-b9f1859e61ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train examples 151978\n",
      "total test examples 74856\n"
     ]
    }
   ],
   "source": [
    "X = df.text.tolist()\n",
    "y = df.label.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "mogTwRfnX-tK"
   },
   "outputs": [],
   "source": [
    "with open('data.train.txt', 'w+') as outfile:\n",
    "    for i in range(len(X_train)):\n",
    "        outfile.write('__label__' + y_train[i] + ' '+ X_train[i] + '\\n')\n",
    "    \n",
    "\n",
    "with open('test.txt', 'w+') as outfile:\n",
    "    for i in range(len(X_test)):\n",
    "        outfile.write('__label__' + y_test[i] + ' ' + X_test[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "u0rt2mhjX-tL",
    "outputId": "fee049fd-e505-483b-970f-3e022acaad38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.8373944640376189\n",
      "R@1: 0.8373944640376189\n",
      "Number of examples: 74856\n"
     ]
    }
   ],
   "source": [
    "classifier = fasttext.train_supervised('data.train.txt')\n",
    "result = classifier.test('test.txt')\n",
    "\n",
    "print('P@1:', result[1])\n",
    "print('R@1:', result[2])\n",
    "print('Number of examples:', result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkb-ckInX-tM"
   },
   "source": [
    "## Задание\n",
    "\n",
    "1. Мы будем работать с данными fakenews отсюда: https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/Constraint_Train.csv\n",
    "2. Проведите препроцессинг текста. Разбейте данные на train и test для задачи классификации.\n",
    "3. Обучите свою модель w2v (или возьмите любую подходящую предобученную модель). Реализуйте функцию для вычисления вектора текста. \n",
    "4. Обучите на полученных средних векторах алгоритм классификации.\n",
    "\n",
    "Бонус: Модифицируйте функцию вычисления среднего вектора: взвешивайте вектора слов соответствующими весами tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "bLJXu8KKX-tM",
    "outputId": "ba48f480-2d13-4c3a-a6d7-9378a723a2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to ru_GB.\n",
      "Warning: Failed to set locale category LC_TIME to ru_GB.\n",
      "Warning: Failed to set locale category LC_COLLATE to ru_GB.\n",
      "Warning: Failed to set locale category LC_MONETARY to ru_GB.\n",
      "Warning: Failed to set locale category LC_MESSAGES to ru_GB.\n",
      "--2021-10-15 13:46:15--  https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/Constraint_Train.csv\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 1253562 (1.2M) [text/plain]\n",
      "Сохранение в: «Constraint_Train.csv»\n",
      "\n",
      "Constraint_Train.cs 100%[===================>]   1.20M  5.16MB/s    за 0.2s    \n",
      "\n",
      "2021-10-15 13:46:15 (5.16 MB/s) - «Constraint_Train.csv» сохранён [1253562/1253562]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/diptamath/covid_fake_news/main/data/Constraint_Train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5NSv9Mo9X-tT"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Constraint_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet label\n",
       "0   1  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1   2  States reported 1121 deaths a small rise from ...  real\n",
       "2   3  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4   5  Populous states can generate large case counts...  real"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Sv0wxUXIX-se"
   ],
   "name": "embeddings_sem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
